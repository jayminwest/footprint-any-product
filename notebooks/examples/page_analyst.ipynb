{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Page Analyst "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "from firecrawl import FirecrawlApp\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import base64\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "# Load environment variables from .env.local file\n",
    "env_path = Path('../..') / '.env.local'\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "api_key = os.environ[\"FIRECRAWL_API_KEY\"]\n",
    "image_link_regex = r\"https?://\\S+\\.(?:jpg|jpeg|png|gif|svg)(?:\\?[\\w=&]*)?\"\n",
    "category_question = \"In less than 10 words and ignoring the brand, what category of product does this detail page sell?\"\n",
    "image_question = '''Extract all image urls from the following markdown, making sure to avoid small icons and logos, and only include the images that pertain to the main product on the page, rather than images of recommended or similar products in other components of the page. Prefer larger images (over 1kb), prefer to be stricter and only keep 1-2 images with white-only backgrounds if possible, rather than keeping too many or images with complex backgrounds. Return the result as a space-delimited list of image_urls and nothing else.'''\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\")\n",
    "\n",
    "def trim_url(url):\n",
    "    parsed = urlparse(url)\n",
    "    return parsed.scheme + '://' + parsed.netloc + parsed.path\n",
    "\n",
    "class PageAnalyzer:\n",
    "    url = None\n",
    "    markdown = None\n",
    "    image_urls = []\n",
    "    category = None\n",
    "    images = {}\n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.url = trim_url(url)\n",
    "        self._do_scrape()\n",
    "        # self._check_or_scrape()\n",
    "\n",
    "    def _do_scrape(self):\n",
    "        # scrape the page\n",
    "        self.markdown = FirecrawlApp(api_key=api_key).scrape_url(self.url, formats=['markdown']).markdown\n",
    "        print(f\"Scraped markdown for {self.url}\")\n",
    "\n",
    "        # extract the image urls\n",
    "        image_urls_response = llm.invoke(f\"{image_question} \\n\\n {self.markdown}\").content\n",
    "        self.image_urls = re.findall(image_link_regex, image_urls_response)\n",
    "        print(f\"Extracted {len(self.image_urls)} image urls from {self.url}\")\n",
    "\n",
    "        # extract the category\n",
    "        self.category = llm.invoke(f\"{category_question} \\n\\n {self.markdown}\").content\n",
    "        self.category = re.sub(r'[^a-zA-Z\\s]', '', self.category).lower()\n",
    "        print(f\"Extracted category '{self.category}' from {self.url}\")\n",
    "\n",
    "    def query_markdown(self, question):\n",
    "        return llm.invoke(f\"{question} \\n\\n {self.markdown}\").content\n",
    "    \n",
    "    def query_images(self, question):\n",
    "        base64_images = [base64.b64encode(image).decode('utf-8') for image in self.images.values()]\n",
    "        return llm.invoke([\n",
    "            HumanMessage(content=[\n",
    "                {\"type\": \"text\", \"text\": question},\n",
    "                *[{\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}} for base64_image in base64_images]\n",
    "            ])]).content.lower()\n",
    "\n",
    "    def _check_or_scrape(self):\n",
    "        with utils.pool.connection() as conn:\n",
    "            # Create table if it doesn't exist\n",
    "            conn.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS pagemetadata (\n",
    "                    url TEXT PRIMARY KEY,\n",
    "                    markdown TEXT,\n",
    "                    image_urls TEXT[],\n",
    "                    category TEXT\n",
    "                )\"\"\")\n",
    "            \n",
    "            # Check if URL already exists\n",
    "            with conn.cursor() as cur:\n",
    "                cur.execute(\"SELECT url, markdown, image_urls, category FROM pagemetadata WHERE url = %s\", (self.url,))\n",
    "                result = cur.fetchone()\n",
    "                if not result:\n",
    "                    self._do_scrape()\n",
    "                    # Store document content and metadata separately\n",
    "                    cur.execute(\n",
    "                        \"INSERT INTO pagemetadata (url, markdown, image_urls, category) VALUES (%s, %s, %s, %s)\",\n",
    "                        (self.url, self.markdown, self.image_urls, self.category)\n",
    "                    )\n",
    "                else:\n",
    "                    self.markdown = result[1]\n",
    "                    self.image_urls = result[2]\n",
    "                    self.category = result[3]\n",
    "\n",
    "    def load_relevant_images(self):\n",
    "        for image_url in self.image_urls:\n",
    "            if image_url in self.images:\n",
    "                continue\n",
    "            try:\n",
    "                # Skip small images and icons\n",
    "                if any(x in image_url.lower() for x in ['icon', 'sprite', 'pixel', 'logo']):\n",
    "                    continue\n",
    "                    \n",
    "                # Download image\n",
    "                response = requests.get(image_url)\n",
    "                if response.status_code == 200:\n",
    "                    # Get image size from headers\n",
    "                    content_length = int(response.headers.get('content-length', 0))\n",
    "                    if content_length < 1000:  # Skip images smaller than 1KB\n",
    "                        continue\n",
    "                                        \n",
    "                    self.images[image_url] = response.content                        \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image {image_url}: {str(e)}\")\n",
    "                continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped markdown for https://www.walmart.com/ip/Five-Star-1-Subject-7x5-Notebook-Green/5488604868\n",
      "Extracted 2 image urls from https://www.walmart.com/ip/Five-Star-1-Subject-7x5-Notebook-Green/5488604868\n",
      "Extracted category 'notebooks and writing supplies' from https://www.walmart.com/ip/Five-Star-1-Subject-7x5-Notebook-Green/5488604868\n",
      "['https://i5.walmartimages.com/seo/Five-Star-1-Subject-7x5-Notebook-Green_ddf50e5a-77b3-4212-8417-c01c97a00052.26661a0501d10a466cc2229cc4d14d51.jpeg?odnHeight=640&odnWidth=640&odnBg=FFFFFF', 'https://i5.walmartimages.com/seo/Five-Star-1-Subject-7x5-Notebook-Green_ddf50e5a-77b3-4212-8417-c01c97a00052.26661a0501d10a466cc2229cc4d14d51.jpeg?odnHeight=372&odnWidth=372&odnBg=FFFFFF']\n"
     ]
    }
   ],
   "source": [
    "pa = PageAnalyzer('https://www.walmart.com/ip/Five-Star-1-Subject-7x5-Notebook-Green/5488604868?athcpid=5488604868&athpgid=AthenaItempage&athcgid=null&athznid=si&athieid=v0_eeMjEyLjg2LDY2NzAuNTYsMC4wMzE3MDkzNTMwMjk2OTg5NCwwLjVf&athstid=CS055~CS004&athguid=EmUu_FhMExkUy0XMpCa_SLyabAy10fZMsFd4&athancid=5349984957&athposb=0&athena=true&athbdg=L1600')\n",
    "print(pa.image_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.walmart.com/ip/Five-Star-1-Subject-7x5-Notebook-Green/5488604868\n",
      "unknown\n",
      "Unknown\n"
     ]
    }
   ],
   "source": [
    "print(pa.url)\n",
    "print(pa.query_markdown(\"Where is this product is made. Be very very concise. Make sure you're not simply returning the brand of the product, but an actual location where it may have been manufactured. If that cannot be determined, return 'unknown'.\"))\n",
    "print(pa.query_markdown(\"Where is this product is shipping from. Be very very concise. Make sure you're not simply returning the brand of the product, but an actual location where it may be shipping from. If that cannot be determined, return 'unknown'.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.walmart.com/ip/Five-Star-1-Subject-7x5-Notebook-Green/5488604868\n",
      "dict_keys([])\n",
      "hereâ€™s a concise list of raw materials likely used in the production of a college ruled notebook like the five star notebook, along with estimated percentages of total mass:\n",
      "\n",
      "1. **paper** - 70-80%\n",
      "2. **plastic (for cover)** - 10-15%\n",
      "3. **wire (for binding)** - 5-10%\n",
      "4. **ink (for printing)** - 1-5%\n",
      "\n",
      "(note: actual percentages may vary depending on the specific product design and materials used.)\n"
     ]
    }
   ],
   "source": [
    "print(pa.url)\n",
    "pa.load_relevant_images()\n",
    "print(pa.images.keys())\n",
    "print(pa.query_images(\"Provide a very concise list of raw materials that appear to be inputs to this product including the percent of the total mass.\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
